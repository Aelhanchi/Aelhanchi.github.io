I"v<p><img style="float: right;     margin-top: 25px;     margin-bottom: 35px;     max-width: 30%;     border: 6px solid #ddd;     border-radius: 50%;     box-sizing: border-box;" src="files/picture_3.jpg" /></p>

<p>I am a third year Ph.D. student in the <a href="https://web.cs.toronto.edu/">Department of Computer Science</a> at the University of Toronto, supervised by <a href="https://www.cs.toronto.edu/~cmaddis/">Chris Maddison</a> and <a href="https://www.cs.toronto.edu/~erdogdu/">Murat Erdogdu</a>. Previously, I did my M.Sc. degree in the <a href="https://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a> at McGill University under the supervision of <a href="https://www.math.mcgill.ca/dstephens/">David Stephens</a>. Before that, I did my B.Sc. degree in
<a href="https://www.mcgill.ca/mathstat/undergraduate/programs/b-sc/joint-honours-mathematics-and-computer-science-b-sc">Honours Mathematics and Computer Science</a> at McGill University.</p>

<!---[Resume](files/resume.pdf) / [Google Scholar](https://scholar.google.com/citations?user=5ZzcGmgAAAAJ&hl=en&oi=ao)-->

<h3 id="papers">Papers</h3>

<p><strong>Optimal Excess Risk Bounds for Empirical Risk Minimization on p-Norm Linear Regression.</strong><br />
Ayoub El Hanchi, Murat A. Erdogdu.<br />
<em>Advances in Neural Information Processing Systems, 2023. (To appear)</em><br />
<a href="files/paper_4.pdf">paper</a></p>

<p><strong>Contrastive Learning Can Find an Optimal Basis for Approximately View-invariant Functions.</strong><br />
Daniel D. Johnson, Ayoub El Hanchi, Chris J. Maddison.<br />
<em>International Conference on Learning Representations, 2023.</em><br />
<a href="files/paper_3.pdf">paper</a></p>

<p><strong>Stochastic Reweighted Gradient Descent.</strong><br />
Ayoub El Hanchi, David A. Stephens, Chris J. Maddison.<br />
<em>International Conference on Machine Learning, 2022.</em><br />
<a href="files/paper_2.pdf">paper</a></p>

<p><strong>Adaptive Importance Sampling for Finite-Sum Optimization and Sampling with Decreasing Step-Sizes.</strong><br />
Ayoub El Hanchi, David A. Stephens.<br />
<em>Advances in Neural Information Processing Systems, 2020.</em><br />
<a href="files/paper_1.pdf">paper</a> <!---| [slides](files/presentation_1.pdf) | [poster](files/poster_1.pdf)--></p>

<!---
### Notes ###



**A Lyapunov Analysis of Loopless SARAH.**  
Ayoub El Hanchi  
[paper](files/paper_2.pdf)

### Thesis ###
**Large Scale Optimization and Sampling for Machine Learning and Statistics.**  
M.Sc. in Mathematics and Statistics, McGill University, May 2021.  
[thesis](files/thesis_1.pdf)
-->

<!---
### Software ###
**TorchVr (in progress)**  
A PyTorch library providing PyTorch modules and samplers that produce efficient gradient estimators to accelerate training of large scale models.  
All samplers are written in C++ using an efficient tree implementation for increased performance.
The C++ code is then exposed to python
using [pybind11](https://github.com/pybind/pybind11).  
<a href="files/code_1.zip">source

### Old Reports  ###
+ *Langevin Diffusion as Gradient Flow in Wasserstein Space.*  
<a href="files/report_4.pdf">report</a>
+ *Scaling up MCMC for Bayesian inference using adaptive data subsampling.*  
<a href="files/report_3.pdf">report</a> \|
<a href="files/presentation_4.pdf">slides</a>
+ *Statistical learning under a non-iid data generating process.*  
<a href="files/report_2.pdf">report</a>
-->
:ET