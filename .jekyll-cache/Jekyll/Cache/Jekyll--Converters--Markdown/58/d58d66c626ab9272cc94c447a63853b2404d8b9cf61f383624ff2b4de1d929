I"<p><img style="float: right;     margin-top: 25px;     margin-bottom: 35px;     max-width: 30%;     border: 6px solid #ddd;     border-radius: 50%;     box-sizing: border-box;" src="files/picture_3.jpg" /></p>

<p>I am a first year Ph.D. student in the <a href="https://web.cs.toronto.edu/">Department of Computer Science</a> at the University of Toronto, supervised by <a href="https://www.cs.toronto.edu/~cmaddis/">Chris Maddison</a> and <a href="https://www.cs.toronto.edu/~erdogdu/">Murat Erdogdu</a>. Previously, I did my M.Sc. degree in the <a href="https://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a> at McGill University under the supervision of <a href="https://www.math.mcgill.ca/dstephens/">David Stephens</a>. Before that, I did my B.Sc. degree in
<a href="https://www.mcgill.ca/mathstat/undergraduate/programs/b-sc/joint-honours-mathematics-and-computer-science-b-sc">Honours Mathematics and Computer Science</a> at McGill University.</p>

<p>My goal is to develop and refine the computational and statistical tools needed to build agents and systems that make optimal decisions under uncertainty.
On the computational side, I have focused on gradient-based stochastic optimization and sampling algorithms.
On the statistical side, I am currently exploring how the use of intermediate data representations can improve our ability to process and exploit data for optimal decision making.</p>

<p><a href="files/resume.pdf">Resume</a> / <a href="https://scholar.google.com/citations?user=5ZzcGmgAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a></p>

<h3 id="papers">Papers</h3>
<p><strong>Adaptive Importance Sampling for Finite-Sum
Optimization and Sampling with Decreasing Step-Sizes.</strong><br />
Ayoub El Hanchi, David A. Stephens<br />
<em>Advances in Neural Information Processing Systems, 2020</em><br />
<a href="files/paper_1.pdf">paper</a> <!---| [slides](files/presentation_1.pdf) | [poster](files/poster_1.pdf)--></p>

<h3 id="preprints">Preprints</h3>
<p><strong>Stochastic Reweighted Gradient Descent.</strong><br />
Ayoub El Hanchi, David A. Stephens<br />
<a href="files/paper_3.pdf">paper</a></p>

<p><strong>A Lyapunov Analysis of Loopless SARAH.</strong><br />
Ayoub El Hanchi<br />
<a href="files/paper_2.pdf">paper</a></p>

<h3 id="thesis">Thesis</h3>
<p><strong>Large Scale Optimization and Sampling for Machine Learning and Statistics.</strong><br />
M.Sc. in Mathematics and Statistics, McGill University, May 2021.<br />
<a href="files/thesis_1.pdf">thesis</a></p>

<!---
### Software ###
**TorchVr (in progress)**  
A PyTorch library providing PyTorch modules and samplers that produce efficient gradient estimators to accelerate training of large scale models.  
All samplers are written in C++ using an efficient tree implementation for increased performance.
The C++ code is then exposed to python
using [pybind11](https://github.com/pybind/pybind11).  
<a href="files/code_1.zip">source

### Old Reports  ###
+ *Langevin Diffusion as Gradient Flow in Wasserstein Space.*  
<a href="files/report_4.pdf">report</a>
+ *Scaling up MCMC for Bayesian inference using adaptive data subsampling.*  
<a href="files/report_3.pdf">report</a> \|
<a href="files/presentation_4.pdf">slides</a>
+ *Statistical learning under a non-iid data generating process.*  
<a href="files/report_2.pdf">report</a>
-->
:ET