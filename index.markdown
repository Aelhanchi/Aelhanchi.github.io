---
layout: page
---
<img style="float: right;
    margin-top: 25px;
    margin-bottom: 35px;
    max-width: 30%;
    border: 6px solid #ddd;
    border-radius: 50%;
    box-sizing: border-box;"
    src="files/picture_3.jpg">

I am a first year Ph.D. student in the [Department of Computer Science](https://web.cs.toronto.edu/) at the University of Toronto, supervised by [Chris Maddison](https://www.cs.toronto.edu/~cmaddis/) and [Murat Erdogdu](https://www.cs.toronto.edu/~erdogdu/). Previously, I did my M.Sc. degree in the [Department of Mathematics and Statistics](https://www.mcgill.ca/mathstat/) at McGill University under the supervision of [David Stephens](https://www.math.mcgill.ca/dstephens/). Before that, I did my B.Sc. degree in
[Honours Mathematics and Computer Science](https://www.mcgill.ca/mathstat/undergraduate/programs/b-sc/joint-honours-mathematics-and-computer-science-b-sc) at McGill University.

My goal is to develop and refine the computational and statistical tools needed to build agents and systems that make optimal decisions under uncertainty.
On the computational side, I have focused on gradient-based stochastic optimization and sampling algorithms.
On the statistical side, I am currently exploring how the use of intermediate data representations can improve our ability to process and exploit data for optimal decision making.

[Resume](files/resume.pdf) / [Google Scholar](https://scholar.google.com/citations?user=5ZzcGmgAAAAJ&hl=en&oi=ao)


### Papers ###
**Adaptive Importance Sampling for Finite-Sum
Optimization and Sampling with Decreasing Step-Sizes.**  
Ayoub El Hanchi, David A. Stephens  
*Advances in Neural Information Processing Systems, 2020*  
[paper](files/paper_1.pdf) <!---| [slides](files/presentation_1.pdf) | [poster](files/poster_1.pdf)-->

### Working papers ###
**Stochastic Reweighted Gradient Descent.**  
Ayoub El Hanchi, David A. Stephens  
[paper](files/paper_3.pdf)


**A Lyapunov Analysis of Loopless SARAH.**  
Ayoub El Hanchi  
[paper](files/paper_2.pdf)

### Thesis ###
**Large Scale Optimization and Sampling for Machine Learning and Statistics.**  
M.Sc. in Mathematics and Statistics, McGill University, May 2021.  
[thesis](files/thesis_1.pdf)

<!---
### Software ###
**TorchVr (in progress)**  
A PyTorch library providing PyTorch modules and samplers that produce efficient gradient estimators to accelerate training of large scale models.  
All samplers are written in C++ using an efficient tree implementation for increased performance.
The C++ code is then exposed to python
using [pybind11](https://github.com/pybind/pybind11).  
<a href="files/code_1.zip">source

### Old Reports  ###
+ *Langevin Diffusion as Gradient Flow in Wasserstein Space.*  
<a href="files/report_4.pdf">report</a>
+ *Scaling up MCMC for Bayesian inference using adaptive data subsampling.*  
<a href="files/report_3.pdf">report</a> \|
<a href="files/presentation_4.pdf">slides</a>
+ *Statistical learning under a non-iid data generating process.*  
<a href="files/report_2.pdf">report</a>
-->