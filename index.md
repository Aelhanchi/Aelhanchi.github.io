---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

I am a U3 student in Mathematics and Computer Science at McGill University.

I am broadly interested in machine learning, and more specifically in the intersection of theory and algorithms.
I am currently working on approximate Bayesian inference using stochastic gradient MCMC and I am exploring statistical learning theory
in a non-i.i.d. environment.

My work on stochastic gradient MCMC: [Here](/SG MCMC.pdf) (still in progress)

A partial summary of [this](https://cs.nyu.edu/~mohri/pub/nonstat.pdf) paper on non-i.i.d. statistical learning: [Here](/Statistical learning.pdf)

General problems that I am interested in:

## Algorithm design
- Optimizing the hyperparameters of optimization and sampling algorithms.
- Interpretability and uncertainty estimation in deep learning models.
- Developing large scale black-box inference algorithms.

## Theory
- Analysis of convergence of optimization and sampling algorithms.
- Bayesian inference theory.
- Learning in non-i.i.d. environments.

I am currently focused on stochastic gradient based algorithms, specifically in terms of variance reduction and hyperparameter optimization.
